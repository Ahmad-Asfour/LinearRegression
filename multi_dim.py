import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
class Multi_Dim_LR:
    def __init__(self, points, initial_theta, iternum, lr, batch_size):
        '''
        points: 2D numpy array (nxd)
        initial_theta: np array of length d+1
        iternum: int
        lr: float
        '''
        self.points = points
        self.initial_theta = initial_theta
        self.iternum = iternum
        self.lr = lr
        self.batch_size = batch_size
    
    def compute_err(self, points, theta):
        '''
        Computes and returns MSE for a given theta
        '''
        n = len(points)
        bias = theta[0]
        weights = theta[1:]

        mse = (np.sum((points[: , -1] - (points[: , :-1] @ weights + bias))  ** 2 )) / n

        return mse
    
    def gdb_runner(self, points, starting_theta, lr, iternum):
        '''
        goes through all iterations in iternum continuously updating theta
        returns theta
        '''
        theta = starting_theta
        for i in range(iternum):
            theta = self.step_gradient(theta, points, lr)
        return theta
    

    def step_gradient(self, theta, points, lr):
        '''
        Calculates the next step by using the average gradient of a batch of datapoints
            -> basically for each datapoint in the batch we need to calculate each "feature's" (or "weight's") gradient
        '''
        batch_size = self.batch_size
        X = points[:, :-1]
        y = points[:, -1]

        beta = theta[0]
        weights = theta[1:]

        indices = np.random.choice(X.shape[0], size=batch_size, replace=False)
        x_batch = X[indices]
        y_batch = y[indices] 

        y_pred = beta + x_batch @ weights  # Shape (batch_size,)
        err = y_batch - y_pred             # Shape (batch_size,)

        grad_beta = -2 * np.sum(err) / batch_size
        grad_weights = -2 * (x_batch.T @ err) / batch_size  # Shape (num_features,)

        new_beta = beta - lr * grad_beta
        new_weights = weights - lr * grad_weights

        new_theta = np.insert(new_weights, 0, new_beta)

        return new_theta

    def plot_result(self, theta):
        '''
        Plots the original data points and the fitted regression plane.
        '''
        fig = plt.figure()
        ax = fig.add_subplot(111, projection='3d')

        x1 = self.points[:, 0]
        x2 = self.points[:, 1]
        y = self.points[:, -1]
        
        ax.scatter(x1, x2, y, color='blue', label='Data Points')

        x1_mesh, x2_mesh = np.meshgrid(np.linspace(x1.min(), x1.max(), 10),
                                       np.linspace(x2.min(), x2.max(), 10))
        
        bias = theta[0]
        weights = theta[1:]
        y_pred_mesh = bias + weights[0] * x1_mesh + weights[1] * x2_mesh  # y = bias + w1*x1 + w2*x2

        ax.plot_surface(x1_mesh, x2_mesh, y_pred_mesh, color='red', alpha=0.5, rstride=100, cstride=100)

        ax.set_xlabel("Feature 1 (x1)")
        ax.set_ylabel("Feature 2 (x2)")
        ax.set_zlabel("Target (y)")
        ax.legend()
        plt.show()

    def run(self):
        '''
        Runs LR
        '''
        print(f'starting GD at theta = {self.initial_theta} , err={self.compute_err(self.points, self.initial_theta)}')

        theta = self.gdb_runner(self.points, self.initial_theta, self.lr, self.iternum)

        print(f'ending GD at theta = {theta} , err={self.compute_err(self.points, theta)}')
        self.plot_result(theta)
        

if __name__ == '__main__':
    '''
    Testing (This was generated by ChatGPT as a sanity check)
    '''
    # Set random seed for reproducibility
    np.random.seed(42)

    # Generate 100 samples with 3 features
    num_samples = 100
    num_features = 3

    # Randomly generate features
    X = np.random.rand(num_samples, num_features)

    # True parameters for generating the target variable
    true_weights = np.array([3.5, -2.2, 1.7])  # Weights for each feature
    true_bias = 4.0                            # Bias term

    # Generate target variable with a bit of noise
    noise = np.random.normal(0, 0.5, num_samples)  # Adding Gaussian noise
    y = X @ true_weights + true_bias + noise       # Linear relationship

    # Combine features and target into one array
    points = np.hstack((X, y.reshape(-1, 1)))      # Shape (num_samples, num_features + 1)

    # Initial theta (weights and bias)
    initial_theta = np.zeros(num_features + 1)     # Start with zeros

    # Set parameters for training
    iternum = 1000
    lr = 0.01
    batch_size = 20

    # Instantiate and run the model
    model = Multi_Dim_LR(points, initial_theta, iternum, lr, batch_size)
    model.run()

